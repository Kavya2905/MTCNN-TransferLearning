{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "import io\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras import models\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import skimage.draw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications import vgg16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from os.path import isdir\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "\n",
    "from numpy.random import seed \n",
    "seed(42)# keras seed fixing \n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(42)# tensorflow seed fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "def show_detected_face(result, detected, title=\"Face image\"):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(result)\n",
    "    img_desc = plt.gca()\n",
    "    plt.set_cmap('gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if len(detected) > 0:\n",
    "      for patch in detected:\n",
    "          \n",
    "          img_desc.add_patch(\n",
    "              patches.Rectangle(\n",
    "                  (patch['c'], patch['r']),\n",
    "                  patch['width'],\n",
    "                  patch['height'],\n",
    "                  fill=False,\n",
    "                  color='r',\n",
    "                  linewidth=2)\n",
    "          )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, io\n",
    "from skimage.feature import Cascade\n",
    "\n",
    "\n",
    "# Load the trained file from the module root.\n",
    "trained_file = data.lbp_frontal_face_cascade_filename()\n",
    "\n",
    "# Initialize the detector cascade.\n",
    "detector = Cascade(trained_file)\n",
    "\n",
    "face_image = io.imread('Nikhil.jpg')\n",
    "\n",
    "detected = detector.detect_multi_scale(img=face_image,\n",
    "                                       scale_factor=1.2,\n",
    "                                       step_ratio=1,\n",
    "                                       min_size=(80, 80),\n",
    "                                       max_size=(1000, 1000))\n",
    "\n",
    "show_detected_face(face_image, detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(result, detected, title=\"Detected face\"):\n",
    "  if len(detected) >0:\n",
    "      rostro= result[detected[0]['r']:detected[0]['r']+detected[0]['width'], detected[0]['c']:detected[0]['c']+detected[0]['height']]\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      plt.imshow(rostro)    \n",
    "      plt.title(title)\n",
    "      plt.axis('off')\n",
    "      plt.show()\n",
    "\n",
    "print(detected)\n",
    "crop_face(face_image, detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "  # load image from file\n",
    "  image = Image.open(filename)\n",
    "  # convert to RGB, if needed\n",
    "  image = image.convert('RGB')\n",
    "  # convert to array\n",
    "  pixels = asarray(image)\n",
    "  # create the detector, using default weights\n",
    "  detector = MTCNN()\n",
    "  # detect faces in the image\n",
    "  results = detector.detect_faces(pixels)\n",
    "  # extract the bounding box from the first face\n",
    "  try:\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "  except IndexError:\n",
    "    face_array=[]\n",
    "    print(\"Out of index\")\n",
    "\n",
    "# specify folder to plot\n",
    "i = 1\n",
    "folder = 'C:/Users/Kavya/Downloads/Whole dataset-20220413T143752Z-001/Whole dataset/Kavya/'\n",
    "# enumerate files\n",
    "for filename in listdir(folder):\n",
    "  # path\n",
    "  path = folder + filename\n",
    "  # get face\n",
    "  face = extract_face(path)\n",
    "  from matplotlib.pyplot import figure\n",
    "  figure(num=None, figsize=(12, 10))\n",
    "  if face is not None:\n",
    "    print(i, face.shape)\n",
    "    # plot\n",
    "    pyplot.subplot(4, 2, i)\n",
    "    pyplot.title(filename)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(face)\n",
    "    i += 1\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(224, 224)):\n",
    "  # load image from file\n",
    "  image = Image.open(filename)\n",
    "  # convert to RGB, if needed\n",
    "  image = image.convert('RGB')\n",
    "  # convert to array\n",
    "  pixels = asarray(image)\n",
    "  # create the detector, using default weights\n",
    "  detector = MTCNN()\n",
    "  # detect faces in the image\n",
    "  results = detector.detect_faces(pixels)\n",
    "  # extract the bounding box from the first face\n",
    "  try:\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "  except IndexError:\n",
    "    face_array=[]\n",
    "    print(\"Out of index\")\n",
    "  \n",
    " \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "  faces = list()\n",
    "  # enumerate files\n",
    "  for filename in listdir(directory):\n",
    "    # path\n",
    "    path = directory + filename\n",
    "    # get face\n",
    "    face = extract_face(path)\n",
    "    if face is not None:\n",
    "      # store\n",
    "      faces.append(face)\n",
    "  return faces\n",
    " \n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "\tX, y = list(), list()\n",
    "\t# enumerate folders, on per class\n",
    "\tfor subdir in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + subdir + '/'\n",
    "\t\t# skip any files that might be in the dir\n",
    "\t\tif not isdir(path):\n",
    "\t\t\tcontinue\n",
    "\t\t# load all faces in the subdirectory\n",
    "\t\tfaces = load_faces(path)\n",
    "\t\t# create labels\n",
    "\t\tlabels = [subdir for _ in range(len(faces))]\n",
    "\t\t# summarize progress\n",
    "\t\tprint('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "\t\t# store\n",
    "\t\tX.extend(faces)\n",
    "\t\ty.extend(labels)\n",
    "\treturn asarray(X), asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all faces from a given photograph\n",
    "def extract_all_faces(filename, required_size=(224, 224)):\n",
    "  # load image from file\n",
    "  original_image = Image.open(filename)\n",
    "  # convert to RGB, if needed\n",
    "  image = original_image.convert('RGB')\n",
    "  # convert to array\n",
    "  pixels = asarray(image)\n",
    "  # create the detector, using default weights\n",
    "  detector = MTCNN()\n",
    "  # detect faces in the image\n",
    "  results = detector.detect_faces(pixels)\n",
    "  # extract the bounding box from the faces\n",
    "  face_array=[]\n",
    "  try:\n",
    "    for result in results:\n",
    "      x1, y1, width, height = result['box']\n",
    "      x1, y1 = abs(x1), abs(y1)\n",
    "      x2, y2 = x1 + width, y1 + height\n",
    "      # extract the face\n",
    "      face = pixels[y1:y2, x1:x2]\n",
    "      #print()\n",
    "      #print(original_image)\n",
    "      #print(face.shape[0])\n",
    "      #print(\"original_image.size[0]/20\")\n",
    "      #print(original_image.size[0]/20)\n",
    "      if face.shape[0] >= original_image.size[0]/20:\n",
    "        print(face.shape)\n",
    "        # resize pixels to the model size\n",
    "        image = Image.fromarray(face)\n",
    "        image = image.resize(required_size)\n",
    "        face_array.append(asarray(image))\n",
    "\n",
    "    return face_array\n",
    "  except IndexError:\n",
    "    print(\"Out of index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate face training dataset from file folders\n",
    "\n",
    "trainX, trainy = load_dataset('Whole dataset/')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('photos.npz', trainX, trainy)\n",
    "x_train_cam, y_train_cam = data['arr_0'], data['arr_1']    #m\n",
    "print('Loaded: ', x_train_cam.shape, y_train_cam.shape)    #m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate face training dataset from file folders\n",
    "\n",
    "trainX, trainy = load_dataset('Kavya dataset')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# save arrays to one file in compressed format\n",
    "#savez_compressed('Kavya1.npz', trainX, trainy)\n",
    "x_train_kavya, y_train_kavya = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_kavya.shape, y_train_kavya.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate face training dataset from file folders\n",
    "\n",
    "trainX, trainy = load_dataset('Nikhil V Gopal dataset')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('Nikhil.npz',trainX, trainy)\n",
    "x_train_nikhil, y_train_nikhil = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nikhil.shape, y_train_nikhil.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate face training dataset from file folders\n",
    "\n",
    "trainX, trainy = load_dataset('Nikhil M Nair dataset')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('NikhilM.npz',trainX, trainy)\n",
    "x_train_nikhilm, y_train_nikhilm = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nikhilm.shape, y_train_nikhilm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate face training dataset from file folders\n",
    "\n",
    "trainX, trainy = load_dataset('Nithya dataset')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('Nithya.npz',trainX, trainy)\n",
    "x_train_nithya, y_train_nithya = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nithya.shape, y_train_nithya.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('Nikhil.npz')     \n",
    "x_train_nikhil, y_train_nikhil = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nikhil.shape, y_train_nikhil.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset from npz files\n",
    "data = load('photos.npz')     \n",
    "x_train_cam, y_train_cam = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_cam.shape, y_train_cam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_cam, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x_train_cam, x_train_nikhil), axis=0)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('Kavya.npz')     \n",
    "x_train_kavya, y_train_kavya = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_kavya.shape, y_train_kavya.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x_train_cam, x_train_kavya), axis=0)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('NikhilM.npz')     \n",
    "x_train_nikhilm, y_train_nikhilm = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nikhilm.shape, y_train_nikhilm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x_train_cam, x_train_nikhilm), axis=0)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('Nithya.npz')     \n",
    "x_train_nithya, y_train_nithya = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', x_train_nithya.shape, y_train_nithya.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x_train_cam, x_train_nithya), axis=0)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(10,10,figsize=(15,15))\n",
    "for i in range(90):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.title(i*1)\n",
    "    plt.imshow(x_train_kavya[i*1])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('photos.npz')     \n",
    "X, Y = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def brightness(img, low, high):\n",
    "    value = random.uniform(low, high)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hsv = np.array(hsv, dtype = np.float64)\n",
    "    hsv[:,:,1] = hsv[:,:,1]*value\n",
    "    hsv[:,:,1][hsv[:,:,1]>255]  = 255\n",
    "    hsv[:,:,2] = hsv[:,:,2]*value \n",
    "    hsv[:,:,2][hsv[:,:,2]>255]  = 255\n",
    "    hsv = np.array(hsv, dtype = np.uint8)\n",
    "    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_result = brightness(img, 0.1, 2)\n",
    "plt.imshow(img_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Original\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "\n",
    "def augment_data(imgs, scales, rotations, brightness_values, shifts, labels):\n",
    "  timg=[]\n",
    "  y_hat=[]\n",
    "  for (img,label) in zip(imgs,labels):\n",
    "    rows, cols, ch = img.shape \n",
    "    for j in rotations:\n",
    "      f = random.uniform(0, 1)\n",
    "      #f = random.choice([0,1])\n",
    "      rot_mat = cv2.getRotationMatrix2D( (14,14), j, 1);\n",
    "      affined_img = cv2.warpAffine( img, rot_mat, (cols, rows), borderValue=(1,1,1))\n",
    "      resulting_img = affined_img\n",
    "      if f == 1:\n",
    "        resulting_img = cv2.flip(resulting_img, f)\n",
    "      resulting_img = brightness(resulting_img, brightness_values[0], brightness_values[1])\n",
    "      timg.append(resulting_img)\n",
    "      y_hat.append(label)\n",
    "  return np.array(timg), np.array(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = np.arange(0.9, 1, 0.1)\n",
    "rotations = np.arange(0, 2)\n",
    "brightness_values = [ 0.4, 2]\n",
    "shifts = [ 0.2]\n",
    "au_x, au_y = augment_data(X, scales, rotations, brightness_values, shifts, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(au_y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(10,10,figsize=(15,15))\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(au_x[i*26])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train test sets\n",
    "au_x_train, au_x_test, au_y_train, au_y_test = train_test_split(au_x, au_y, test_size=0.33)\n",
    "print(au_x_train.shape, au_x_test.shape, au_y_train.shape, au_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(au_y_train)\n",
    "au_y_train = out_encoder.transform(au_y_train)\n",
    "au_y_test = out_encoder.transform(au_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "au_train_labels = to_categorical(au_y_train)\n",
    "au_test_labels = to_categorical(au_y_test)\n",
    "\n",
    "print(au_train_labels.shape)\n",
    "print(au_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "savez_compressed('data_encoded.npz', au_x_train, au_train_labels, au_x_test, au_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the faces dataset\n",
    "data = load('data_encoded.npz')     \n",
    "au_x_train, au_train_labels, au_x_test, au_test_labels= data['arr_0'], data['arr_1'],data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', au_x_train.shape, au_train_labels.shape,  au_x_test.shape, au_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, GlobalMaxPooling2D\n",
    "from numpy.random import seed \n",
    "seed(42)# keras seed fixing \n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(42)# tensorflow seed fixing\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# Convolution layers\n",
    "model1.add(Conv2D(32, (3,3), input_shape = (224, 224, 3), activation = 'relu', padding='same'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model1.add(MaxPooling2D(2))\n",
    "\n",
    "model1.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(units = 64, activation = 'relu'))\n",
    "model1.add(Dropout(0.2)) \n",
    "\n",
    "# Output\n",
    "model1.add(Dense(units = 4, activation = 'softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "model1.compile(optimizer = 'adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience = 3)\n",
    "checkpoint = ModelCheckpoint('facial_recognition_MTCNN.h5py', monitor = 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_au_x_train = preprocess_input(au_x_train)\n",
    "pre_au_x_test = preprocess_input(au_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(pre_au_x_train, au_train_labels, \n",
    "                     batch_size = 128, epochs=5, \n",
    "                     validation_data = (pre_au_x_test, au_test_labels), \n",
    "                     callbacks = [early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = model1.predict(pre_au_x_test, batch_size=50) # y_test are the truth, real labels correct ones.\n",
    "# classes are the model predictions (That can be wrong or match the real ones)\n",
    "print(\"Total predicted classes are:\",len(preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(au_test_labels.argmax(axis=1), preds1.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred1 = np.argmax(preds1, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(au_test_labels.argmax(axis=1), y_pred1))\n",
    "print('Classification Report')\n",
    "\n",
    "target_names = ['Kavya', 'Nikhil M Nair', 'Nikhil V Gopal', 'Nithya']\n",
    "print(classification_report(au_test_labels.argmax(axis=1), y_pred1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load model\n",
    "model1 = load_model('facial_recognition_MTCNN.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_photo_all_faces_from_path(file_path,verbose=False):\n",
    "  recognized_person = {0:'Kavya',1:'Nikhil M Nair', 2:'Nikhil V Gopal', 3:'Nithya'}\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  original_image = Image.open(file_path)\n",
    "  plt.title(\"Original photo\")\n",
    "  plt.imshow(original_image)\n",
    "  plt.show()\n",
    "  images = extract_all_faces(file_path)\n",
    "  for image in images:\n",
    "    preprocessed_image = preprocess_input(image)\n",
    "    shaped_img = preprocessed_image.reshape(1,224,224,3)\n",
    "   \n",
    "\n",
    "    classes = model1.predict(shaped_img) # y_test are the truth, real labels correct ones.\n",
    "    # classes are the model predictions (That can be wrong or match the real ones)\n",
    "    print(\"Total predicted classes are:\",len(classes))\n",
    "    print(int(classes.argmax(axis=1)))\n",
    "    plt.title(recognized_person[int(classes.argmax(axis=1))])\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "      print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_on_photo_all_faces_from_path(\"Nikhil4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from numpy.random import seed \n",
    "seed(1) # keras seed fixing import\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2) # tensorflow seed fixing\n",
    "\n",
    "model_vgg16 = vgg16.VGG16(include_top=False, pooling=\"max\", input_shape = (224, 224, 3)) \n",
    "\n",
    "new_layer = Dense(256, activation = 'relu')(model_vgg16.output)\n",
    "new_layer = Dropout(rate = 0.5)(new_layer)\n",
    "\n",
    "new_layer = Dense(128, activation = 'relu')(new_layer)\n",
    "new_layer = Dropout(rate = 0.5)(new_layer)\n",
    "\n",
    "new_layer = BatchNormalization()(new_layer)\n",
    "new_layer = Dense(4, activation=\"softmax\")(new_layer)\n",
    "\n",
    "model2 = models.Model(model_vgg16.input, new_layer)\n",
    "\n",
    "for i in range(len(model_vgg16.layers)):\n",
    "  model2.layers[i].trainable = False\n",
    "\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "model2.compile(optimizer = 'adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "pre_au_x_train = preprocess_input(au_x_train)\n",
    "pre_au_x_test = preprocess_input(au_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience = 3)\n",
    "checkpoint = ModelCheckpoint('facial_recongition_VGG16.h5py', monitor = 'val_accuracy', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(pre_au_x_train, au_train_labels, \n",
    "                     batch_size = 128, epochs=5, \n",
    "                     validation_data = (pre_au_x_test, au_test_labels), \n",
    "                     callbacks = [early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict(pre_au_x_test, batch_size=50) # y_test are the truth, real labels correct ones.\n",
    "# classes are the model predictions (That can be wrong or match the real ones)\n",
    "print(\"Total predicted classes are:\",len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(au_test_labels.argmax(axis=1), preds.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(au_y_test, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Kavya', 'Nikhil M Nair', 'Nikhil V Gopal', 'Nithya']\n",
    "print(classification_report(au_y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_model = load_model('facial_recongition_VGG16.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_photo_all_faces_from_path(file_path,verbose=False):\n",
    "  recognized_person = {0:'Kavya',1:'Nikhil M Nair', 2:'Nikhil V Gopal', 3:'Nithya'}\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  original_image = Image.open(file_path)\n",
    "  plt.title(\"Original photo\")\n",
    "  plt.imshow(original_image)\n",
    "  plt.show()\n",
    "  images = extract_all_faces(file_path)\n",
    "  for image in images:\n",
    "    preprocessed_image = preprocess_input(image)\n",
    "    shaped_img = preprocessed_image.reshape(1,224,224,3)\n",
    "    classes = model2.predict(shaped_img) # y_test are the truth, real labels correct ones.\n",
    "    # classes are the model predictions (That can be wrong or match the real ones)\n",
    "    print(\"Total predicted classes are:\",len(classes))\n",
    "    print(int(classes.argmax(axis=1)))\n",
    "    plt.title(recognized_person[int(classes.argmax(axis=1))])\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "      print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_on_photo_all_faces_from_path(\"Nikhil4.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
